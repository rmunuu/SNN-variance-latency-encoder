{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch 1\n",
      "[100, 500] loss: 2.303\n",
      "[200, 500] loss: 2.303\n",
      "[300, 500] loss: 2.303\n",
      "[400, 500] loss: 2.303\n",
      "[500, 500] loss: 2.303\n",
      "Accuracy: 10.00%\n",
      "Epoch 2\n",
      "[100, 500] loss: 2.303\n",
      "[200, 500] loss: 2.303\n",
      "[300, 500] loss: 2.303\n",
      "[400, 500] loss: 2.303\n",
      "[500, 500] loss: 2.303\n",
      "Accuracy: 10.00%\n",
      "Epoch 3\n",
      "[100, 500] loss: 2.303\n",
      "[200, 500] loss: 2.303\n",
      "[300, 500] loss: 2.303\n",
      "[400, 500] loss: 2.303\n",
      "[500, 500] loss: 2.303\n",
      "Accuracy: 10.00%\n",
      "Epoch 4\n",
      "[100, 500] loss: 2.303\n",
      "[200, 500] loss: 2.303\n",
      "[300, 500] loss: 2.303\n",
      "[400, 500] loss: 2.303\n",
      "[500, 500] loss: 2.303\n",
      "Accuracy: 10.00%\n",
      "Epoch 5\n",
      "[100, 500] loss: 2.303\n",
      "[200, 500] loss: 2.303\n",
      "[300, 500] loss: 2.303\n",
      "[400, 500] loss: 2.303\n",
      "[500, 500] loss: 2.303\n",
      "Accuracy: 10.00%\n",
      "Epoch 6\n",
      "[100, 500] loss: 2.303\n",
      "[200, 500] loss: 2.303\n",
      "[300, 500] loss: 2.303\n",
      "[400, 500] loss: 2.303\n",
      "[500, 500] loss: 2.303\n",
      "Accuracy: 10.00%\n",
      "Epoch 7\n",
      "[100, 500] loss: 2.303\n",
      "[200, 500] loss: 2.303\n",
      "[300, 500] loss: 2.303\n",
      "[400, 500] loss: 2.303\n",
      "[500, 500] loss: 2.303\n",
      "Accuracy: 10.00%\n",
      "Epoch 8\n",
      "[100, 500] loss: 2.303\n",
      "[200, 500] loss: 2.303\n",
      "[300, 500] loss: 2.303\n",
      "[400, 500] loss: 2.303\n",
      "[500, 500] loss: 2.303\n",
      "Accuracy: 10.00%\n",
      "Epoch 9\n",
      "[100, 500] loss: 2.303\n",
      "[200, 500] loss: 2.303\n",
      "[300, 500] loss: 2.303\n",
      "[400, 500] loss: 2.303\n",
      "[500, 500] loss: 2.303\n",
      "Accuracy: 10.00%\n",
      "Epoch 10\n",
      "[100, 500] loss: 2.303\n",
      "[200, 500] loss: 2.303\n",
      "[300, 500] loss: 2.303\n",
      "[400, 500] loss: 2.303\n",
      "[500, 500] loss: 2.303\n",
      "Accuracy: 10.00%\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from norse.torch.module.lif import LIFCell\n",
    "\n",
    "# Define the fully Spiking Neural Network\n",
    "class FullySpikingNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(FullySpikingNN, self).__init__()\n",
    "        \n",
    "        # Spiking layers\n",
    "        self.lif1 = LIFCell(input_size=32 * 32 * 3, hidden_size=1024)\n",
    "        self.lif2 = LIFCell(input_size=1024, hidden_size=512)\n",
    "        self.lif3 = LIFCell(input_size=512, hidden_size=256)\n",
    "        # self.lif4 = LIFCell(input_size=256, hidden_size=num_classes)\n",
    "        self.fc = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten the input image\n",
    "        x = x.view(-1, 32 * 32 * 3)\n",
    "        \n",
    "        # Initialize the state for each LIF layer\n",
    "        batch_size = x.size(0)\n",
    "        lif1_state = self.lif1.initial_state(batch_size=batch_size, device=x.device)\n",
    "        lif2_state = self.lif2.initial_state(batch_size=batch_size, device=x.device)\n",
    "        lif3_state = self.lif3.initial_state(batch_size=batch_size, device=x.device)\n",
    "        # lif4_state = self.lif4.initial_state(batch_size=batch_size, device=x.device)\n",
    "        \n",
    "        # Fully spiking layers\n",
    "        z, lif1_state = self.lif1(x, lif1_state)\n",
    "        z, lif2_state = self.lif2(z, lif2_state)\n",
    "        z, lif3_state = self.lif3(z, lif3_state)\n",
    "        # z, lif4_state = self.lif4(z, lif4_state)\n",
    "        x = self.fc(z)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Training and Evaluation\n",
    "def train_snn(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:    # Print every 100 mini-batches\n",
    "            print(f'[{i + 1}, {len(train_loader)}] loss: {running_loss / 100:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "def test_snn(model, test_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Accuracy: {100 * correct / total:.2f}%')\n",
    "\n",
    "\n",
    "# Load CIFAR-10 data\n",
    "transform = transforms.Compose(\n",
    "    [transforms.RandomHorizontalFlip(),\n",
    "     transforms.RandomCrop(32, padding=4),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))])\n",
    "\n",
    "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(trainset, batch_size=100, shuffle=True)\n",
    "\n",
    "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(testset, batch_size=100, shuffle=False)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize the model, criterion, and optimizer\n",
    "model = FullySpikingNN(num_classes=10).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(10):  # Loop over the dataset multiple times\n",
    "    print(f'Epoch {epoch + 1}')\n",
    "    train_snn(model, train_loader, criterion, optimizer, device)\n",
    "    test_snn(model, test_loader, device)\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
